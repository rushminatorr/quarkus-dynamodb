AWSTemplateFormatVersion: "2010-09-09"
Parameters:
  DynamoDBTableName:
    Description: Name of the dynamoDB table you will use
    Type: String
    Default: consumption_data
    ConstraintDescription: must be a valid dynamoDB name.
  BucketName:
    Description: Name of the S3 bucket you will deploy the CSV file to
    Type: String
    Default: rushrush
    ConstraintDescription: must be a valid bucket name.
Resources: 
  myDynamoDBTable: 
    Type: AWS::DynamoDB::Table
    Properties: 
      TableName: !Ref DynamoDBTableName
      AttributeDefinitions: 
        - AttributeName: "meter"
          AttributeType: "S"
        - AttributeName: "date"
          AttributeType: "S"
      KeySchema: 
        - AttributeName: "meter"
          KeyType: "HASH"
        - AttributeName: "date"
          KeyType: "RANGE"
      ProvisionedThroughput: 
        ReadCapacityUnits: "5"
        WriteCapacityUnits: "5"
  S3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Ref BucketName
      AccessControl: BucketOwnerFullControl
  LambdaRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
                - s3.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Path: /
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
        - 'arn:aws:iam::aws:policy/AWSLambdaInvocation-DynamoDB'
        - 'arn:aws:iam::aws:policy/AmazonS3FullAccess'
      Policies:
        - PolicyName: policyname
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Resource: '*'
                Action:
                  - 'dynamodb:PutItem'
                  - 'dynamodb:BatchWriteItem'

  CsvToDDBLambdaFunction:
    Type: 'AWS::Lambda::Function'
    Properties:
      Handler: index.lambda_handler
      Role: !GetAtt 
        - LambdaRole
        - Arn
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          import csv
          import codecs
          import sys
          
          s3 = boto3.resource('s3')
          dynamodb = boto3.resource('dynamodb')
          
          bucketName = os.environ['bucket']
          tableName = os.environ['table']
          
          def lambda_handler(event, context):
          
            batch_size = 100
            batch = []
          
            #get() does not store in memory
            try:
                print("listing s3 objects")
                bucket = s3.Bucket('rushrush')
                print("found s3 objects")
                for obj in bucket.objects.all():
                    print(obj)
                    #DictReader is a generator; not stored in memory
                    data = obj.get()['Body']
                    #DictReader is a generator; not stored in memory
                    for row in csv.DictReader(codecs.getreader('utf-8')(data)):
                        if len(batch) >= batch_size:
                            write_to_dynamo(batch)
                            batch.clear()
                        batch.append(row)
                    if batch:
                        write_to_dynamo(batch)
                    try:
                        print("File processed, will delete it now.")
                        obj.delete()
                    except:
                        print("Error deleting s3 object.")      
            except:
                print("Error Error!! HALP!")
          
          def write_to_dynamo(rows):
            try:
                table = dynamodb.Table("consumption_data")
            except:
                print("Error loading DynamoDB table. Check if table was created correctly and environment variable.")
          
            try:
                with table.batch_writer() as batch:
                    for i in range(len(rows)):
                        print("inserting data...")
                        item = json.loads(json.dumps(rows[i]))
                        batch.put_item(Item=item) 
            except:
              print("Error executing batch_writer")
      Runtime: python3.7
      Timeout: 900
      MemorySize: 3008
      Environment:
        Variables:
          bucket: !Ref BucketName
          table: !Ref DynamoDBTableName
  ScheduledRule: 
    Type: AWS::Events::Rule
    Properties: 
      Description: "ScheduledRule"
      ScheduleExpression: "cron(0 6,19 * * ? *)"
      State: "ENABLED"
      Targets: 
        - 
          Arn: 
            Fn::GetAtt: 
              - "CsvToDDBLambdaFunction"
              - "Arn"
          Id: "CsvToDDBLambdaFunction"
  PermissionForEventsToInvokeLambda: 
    Type: AWS::Lambda::Permission
    Properties: 
      FunctionName: !GetAtt CsvToDDBLambdaFunction.Arn
      Action: "lambda:InvokeFunction"
      Principal: "events.amazonaws.com"
      SourceArn: !GetAtt ScheduledRule.Arn
Outputs: {}
        